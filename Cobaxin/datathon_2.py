# -*- coding: utf-8 -*-
"""Datathon-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D0mz0qX4jHIhi5srmWLlkFQdYdHS2MG4
"""

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import folium
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import KMeans
import xgboost as xgb
import warnings
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

print("OK.")

try:
    df_test = pd.read_csv('DIM_TIENDA_TEST.csv')
    df_train = pd.read_csv('DIM_TIENDA.csv')
    df_meta = pd.read_csv('Meta_venta.csv')
    df_venta = pd.read_csv('Venta.csv')
    print("Data loaded successfully.")

    print("\n--- Columns in df_train ---")
    print(df_train.columns) # Keep this for future checks!
    print("\n--- Columns in df_test ---")
    print(df_test.columns) # Keep this for future checks!

except FileNotFoundError as e:
    print(f"Error {e}. ")
    exit()

# Prepare Meta and Venta data
df_venta['VENTA_TOTAL'] = pd.to_numeric(df_venta['VENTA_TOTAL'], errors='coerce')
df_meta.columns = ['ENTORNO_DES', 'META_VENTA']

lat_min, lat_max = 24.85727, 26.40152
lon_min, lon_max = -100.41022, -98.11861

print(f"\nOriginal: {df_train.shape}")
print(f"Original: {df_test.shape}")

df_train_filtered = df_train[
    (df_train['LATITUD_NUM'] >= lat_min) & (df_train['LATITUD_NUM'] <= lat_max) &
    (df_train['LONGITUD_NUM'] >= lon_min) & (df_train['LONGITUD_NUM'] <= lon_max)
].copy()

df_test_filtered = df_test[
    (df_test['LATITUD_NUM'] >= lat_min) & (df_test['LATITUD_NUM'] <= lat_max) &
    (df_test['LONGITUD_NUM'] >= lon_min) & (df_test['LONGITUD_NUM'] <= lon_max)
].copy()

print(f"Filtered_train: {df_train_filtered.shape}")
print(f"Filtered_test: {df_test_filtered.shape}")

def preprocess(df_base, venta_df, meta_df):

    categorical_cols = ['NIVELSOCIOECONOMICO_DES', 'ENTORNO_DES',
                        'SEGMENTO_MAESTRO_DESC', 'LID_UBICACION_TIENDA']

    # Ensure all categorical columns exist before encoding
    existing_cats = [col for col in categorical_cols if col in df_base.columns]
    df_encoded = pd.get_dummies(df_base, columns=existing_cats, dummy_na=True) # dummy_na handles potential NaNs

    valid_ids = df_encoded['TIENDA_ID'].unique()
    venta_filtered = venta_df[venta_df['TIENDA_ID'].isin(valid_ids)]
    venta_avg = venta_filtered.groupby('TIENDA_ID')['VENTA_TOTAL'].mean().reset_index()

    df_merged = pd.merge(df_encoded, venta_avg, on='TIENDA_ID', how='left')
    # Use original df_base for ENTORNOS_DES for merging meta
    df_merged = pd.merge(df_merged, df_base[['TIENDA_ID', 'ENTORNO_DES']], on='TIENDA_ID', how='left')
    df_merged = pd.merge(df_merged, meta_df, on='ENTORNO_DES', how='left')

    # Handle potential NaNs after merge (fill with 0 or mean/median if appropriate)
    df_merged['VENTA_TOTAL'].fillna(0, inplace=True)
    df_merged['META_VENTA'].fillna(df_merged['META_VENTA'].mean(), inplace=True)

    # Calculate performance, handle potential division by zero
    df_merged['PERFORMANCE'] = np.where(df_merged['META_VENTA'] > 0,
                                        df_merged['VENTA_TOTAL'] / df_merged['META_VENTA'],
                                        0)
    # Handle NaNs in performance (if any remain)
    df_merged['PERFORMANCE'].fillna(0, inplace=True)

    return df_merged

df_processed_train = preprocess(df_train_filtered, df_venta, df_meta)
df_processed_test = preprocess(df_test_filtered, df_venta, df_meta)
print("Ok")

coords_train = df_processed_train[['LATITUD_NUM', 'LONGITUD_NUM']].copy()
coords_test = df_processed_test[['LATITUD_NUM', 'LONGITUD_NUM']].copy()

coords_train.dropna(inplace=True)
coords_test.dropna(inplace=True)

df_processed_train = df_processed_train.loc[coords_train.index]
df_processed_test = df_processed_test.loc[coords_test.index]

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(coords_train.values)
X_test_scaled  = scaler.transform(coords_test.values)


N_CLUSTERS = 5
kmeans = KMeans(n_clusters=N_CLUSTERS,
                random_state=13,
                n_init="auto")  # "auto" or 10 both fine

df_processed_train["CLUSTER_ID"] = kmeans.fit_predict(X_train_scaled)
df_processed_test ["CLUSTER_ID"] = kmeans.predict(X_test_scaled)

print(f"{N_CLUSTERS} clusters.")

X_cluster = coords_train.values


scaler = StandardScaler()
X_cluster = scaler.fit_transform(X_cluster)


inertia = []
k_range = range(1, 11)

for k in k_range:
    km = KMeans(
        n_clusters=k,
        random_state=42,
        n_init="auto",
        max_iter=300
    )
    km.fit(X_cluster)
    inertia.append(km.inertia_)

plt.figure(figsize=(6,4))
plt.plot(k_range, inertia, "bo-")
plt.xlabel("k (number of clusters)")
plt.ylabel("WCSS / Inertia")
plt.title("Elbow method for optimal k")
plt.show()

sil = []
for k in range(2, 12):
    km = KMeans(n_clusters=k, random_state=42, n_init="auto")
    labels = km.fit_predict(X_cluster)
    sil.append(silhouette_score(X_cluster, labels))

plt.figure(figsize=(6,4))
plt.plot(range(2, 12), sil, "ro-")
plt.xlabel("k"), plt.ylabel("Silhouette")
plt.title("Silhouette vs. k")
plt.show()

cluster_means = df_processed_train.groupby('CLUSTER_ID')['PERFORMANCE'].mean().reset_index()
cluster_means.rename(columns={'PERFORMANCE': 'CLUSTER_MEAN'}, inplace=True)


df_final_train = pd.merge(df_processed_train, cluster_means, on='CLUSTER_ID', how='left')
df_final_test = pd.merge(df_processed_test, cluster_means, on='CLUSTER_ID', how='left')

global_mean = df_final_train['PERFORMANCE'].mean()
df_final_test['CLUSTER_MEAN'].fillna(global_mean, inplace=True)


df_final_train['TARGET'] = (df_final_train['PERFORMANCE'] >= df_final_train['CLUSTER_MEAN']).astype(int)
df_final_test['TARGET'] = (df_final_test['PERFORMANCE'] >= df_final_test['CLUSTER_MEAN']).astype(int)


df_final_train = pd.get_dummies(df_final_train, columns=['CLUSTER_ID'], prefix='Cluster')
df_final_test = pd.get_dummies(df_final_test, columns=['CLUSTER_ID'], prefix='Cluster')


drop_cols = ['TIENDA_ID', 'VENTA_TOTAL', 'META_VENTA', 'TARGET',
             'ENTORNO_DES', 'PERFORMANCE', 'DATASET', 'CLUSTER_MEAN']

X = df_final_train.drop(columns=drop_cols, errors='ignore')
y = df_final_train['TARGET']

X_test_final = df_final_test.drop(columns=drop_cols, errors='ignore')
y_test_final = df_final_test['TARGET']

X = X.select_dtypes(include=np.number)
X_test_final = X_test_final.select_dtypes(include=np.number)


X, X_test_final = X.align(X_test_final, join='inner', axis=1, fill_value=0)
print("Ok")

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y)

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test_final)


X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_final.columns)
print("Ok")

from sklearn.metrics import classification_report
import xgboost as xgb


scale_weight = y_train.value_counts()[0] / y_train.value_counts()[1]
print(f"Scale Pos Weight: {scale_weight:.2f}")


param_grid = {
    'n_estimators'     : [300, 600],
    'learning_rate'    : [0.02, 0.05, 0.1],
    'max_depth'        : [4, 6],
    'min_child_weight' : [1, 3],
    'gamma'            : [0, 0.1],
    'subsample'        : [0.7, 0.9],
    'colsample_bytree' : [0.7, 0.9],
}


xgb_base = xgb.XGBClassifier(
    scale_pos_weight = scale_weight,
    objective        = 'binary:logistic',
    eval_metric      = 'aucpr',
    random_state     = 42,
    tree_method      = 'hist'
)

from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(
    estimator = xgb_base,
    param_grid = param_grid,
    scoring = 'f1_weighted',
    cv = 3,
    n_jobs = -1,
    verbose = 2
)

grid_search.fit(X_train_scaled, y_train)

print("\nâœ…  Best Parameters:", grid_search.best_params_)
print(f"Best weighted-F1 (CV): {grid_search.best_score_:.4f}")

best_model = grid_search.best_estimator_

print("\n--- Validation Set ---")
print(classification_report(y_val, best_model.predict(X_val_scaled)))

print("\n--- Test Set ---")
print(classification_report(y_test_final, best_model.predict(X_test_scaled)))

print("\nGenerating cluster map (store_clusters.html)...")
map_center = [(26.40152 + 24.85727) / 2, (-98.11861 + -100.41022) / 2]
cluster_map = folium.Map(location=map_center, zoom_start=9)

colors_map_base = matplotlib.colormaps.get_cmap('tab10')

sample_df = df_final_train.sample(min(1000, len(df_final_train)), random_state=13)

def get_cluster_id(row, n_clusters):
    for i in range(n_clusters):
        if f'Cluster_{i}' in row and row[f'Cluster_{i}'] == 1:
            return i
    return -1

for idx, row in sample_df.iterrows():
    lat, lon = row['LATITUD_NUM'], row['LONGITUD_NUM']
    cluster_id = get_cluster_id(row, N_CLUSTERS)

    if cluster_id != -1:

        color_rgba = colors_map_base(cluster_id / N_CLUSTERS)

        color_hex = matplotlib.colors.to_hex(color_rgba)


        tienda_id_val = row.get('TIENDA_ID', 'N/A')
        performance_val = row.get('PERFORMANCE', 'N/A')


        try:
            perf_text = f"{float(performance_val):.2f}"
        except (ValueError, TypeError):
            perf_text = "N/A"

        popup_text = f"ID: {tienda_id_val}<br>Cluster: {cluster_id}<br>Perf: {perf_text}<br>Target: {row['TARGET']}"

        folium.CircleMarker(
            location=[lat, lon],
            radius=5,
            color=color_hex,
            fill=True,
            fill_color=color_hex,
            fill_opacity=0.7,
            popup=popup_text
        ).add_to(cluster_map)

cluster_map.save("store_clusters.html")
print("Map saved. Open 'store_clusters.html' in your browser.")
print("\n--- Code Execution Finished ---")